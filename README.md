# 🚀 big-data-pipeline-sparky  
A collaborative data science project built for course credit — and for fun! 🧠💡  
Here we explore some big data tools like Apache Spark, MongoDB, and Google Colab, using the TMDB movie dataset as our playground 🎬  

---

## 📚 Table of Contents

### 📁 [Spark Dataframe](spark-dataframe) – Main project folder  
Contains everything from data prep to final integration.

- 📔 [final-notebook-version](spark-dataframe/final-notebook-version)  
  ➡️ Final Notebook Version (full-pipeline): Can be imported into Colab and executed
- 🔗 [colab-mongo-spark-dataview-integration](spark-dataframe/colab-mongo-spark-dataview-integration)  
  ➡️ Full pipeline: Colab + Spark + MongoDB + Dataview  
- 🧹 [data-manipulation](spark-dataframe/data-manipulation)  
  ➡️ Data cleaning, formatting, and prepping  
- 📊 [data-views](spark-dataframe/data-views)  
  ➡️ Visualization experiments and data exploration  
- 🧪 [google-colab-testing](spark-dataframe/google-colab-testing)  
  ➡️ Testbed for Spark, MongoDB, and Drive integration in Colab

---

## 🔧 Project Links

- 🐞 [Project Issues](https://github.com/psword/big-data-pipeline-sparky/issues) – Log bugs, improvements, or questions  
- 📋 [Project Log](https://github.com/users/psword/projects/3/views/1) – Progress and planning board

---

## 👥 Contributors  
- *️⃣ [Satu-source](https://github.com/Satu-source)
  📊 Data views and loading into Mongo
- *️⃣ [JKin-0](https://github.com/JKin-0)
  🧹 Data preparation and filtering
- *️⃣ [psword](https://github.com/psword)
  🧪 Colab testing between and GitHub host
---

## The driver :car: for this repository:

📚Final Project: Big Data Pipeline with Apache Spark & MongoDB

🚀In final project, you will design and implement a Big Data processing pipeline using Apache Spark and MongoDB. The project will involve ingesting, processing, analyzing, and storing large-scale datasets to derive meaningful insights. You will demonstrate their understanding of data engineering, query optimization, and integration between Spark and MongoDB.

#### <ins>Requirements</ins>

Each team must:

✔ Choose a real-world dataset (e.g., Social Media, IoT, Financial, Healthcare, E-Commerce).

✔ Store the dataset in MongoDB, utilizing an optimized schema design.

✔ Process and analyze the data using Apache Spark (PySpark).

✔ Use Spark SQL for querying the dataset, comparing performance with MongoDB Aggregation Framework.

✔ Optimize performance using indexing, sharding, and partitioning techniques in MongoDB.

✔ Visualize key insights using Python libraries such as Matplotlib or Seaborn.

✔ Present findings in a final report and a class presentation.

#### <ins>Deliverables</ins>

__Project Proposal (1-page summary) – Dataset choice, objectives, expected outcomes. (Due: Week 3)__

- Implementation & Codebase – A well-structured code repository (GitHub).
- Final Report (3-5 pages) – Introduction, methodology, implementation, results, and conclusions.
- Final Presentation (10-15 minutes on ) – Demonstration of the pipeline and key insights.
