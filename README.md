# big-data-pipeline-sparky
Collaborative project for a data science course.


## Here is the driver for this repository:

ðŸ“šFinal Project: Big Data Pipeline with Apache Spark & MongoDB

ðŸš€In final project, you will design and implement a Big Data processing pipeline using Apache Spark and MongoDB. The project will involve ingesting, processing, analyzing, and storing large-scale datasets to derive meaningful insights. You will demonstrate their understanding of data engineering, query optimization, and integration between Spark and MongoDB.

#### <ins>Requirements</ins>

Each team must:

âœ” Choose a real-world dataset (e.g., Social Media, IoT, Financial, Healthcare, E-Commerce).

âœ” Store the dataset in MongoDB, utilizing an optimized schema design.

âœ” Process and analyze the data using Apache Spark (PySpark).

âœ” Use Spark SQL for querying the dataset, comparing performance with MongoDB Aggregation Framework.

âœ” Optimize performance using indexing, sharding, and partitioning techniques in MongoDB.

âœ” Visualize key insights using Python libraries such as Matplotlib or Seaborn.

âœ” Present findings in a final report and a class presentation.

#### <ins>Deliverables</ins>

__Project Proposal (1-page summary) â€“ Dataset choice, objectives, expected outcomes. (Due: Week 3)__

- Implementation & Codebase â€“ A well-structured code repository (GitHub).
- Final Report (3-5 pages) â€“ Introduction, methodology, implementation, results, and conclusions.
- Final Presentation (10-15 minutes on ) â€“ Demonstration of the pipeline and key insights.
