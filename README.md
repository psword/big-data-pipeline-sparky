# ğŸš€ big-data-pipeline-sparky  
A collaborative data science project built for course credit â€” and for fun! ğŸ§ ğŸ’¡  
Here we explore some big data tools like Apache Spark, MongoDB, and Google Colab, using the TMDB movie dataset as our playground ğŸ¬  

---

## ğŸ“š Table of Contents

### ğŸ“ [Spark Dataframe](spark-dataframe) â€“ Main project folder  
Contains everything from data prep to final integration.

- ğŸ“” [final-notebook-version](spark-dataframe/final-notebook-version)  
  â¡ï¸ Final Notebook Version (full-pipeline): Can be imported into Colab and executed
- ğŸ”— [colab-mongo-spark-dataview-integration](spark-dataframe/colab-mongo-spark-dataview-integration)  
  â¡ï¸ Full pipeline: Colab + Spark + MongoDB + Dataview  
- ğŸ§¹ [data-manipulation](spark-dataframe/data-manipulation)  
  â¡ï¸ Data cleaning, formatting, and prepping  
- ğŸ“Š [data-views](spark-dataframe/data-views)  
  â¡ï¸ Visualization experiments and data exploration  
- ğŸ§ª [google-colab-testing](spark-dataframe/google-colab-testing)  
  â¡ï¸ Testbed for Spark, MongoDB, and Drive integration in Colab

---

## ğŸ”§ Project Links

- ğŸ [Project Issues](https://github.com/psword/big-data-pipeline-sparky/issues) â€“ Log bugs, improvements, or questions  
- ğŸ“‹ [Project Log](https://github.com/users/psword/projects/3/views/1) â€“ Progress and planning board

---

## ğŸ‘¥ Contributors  
- *ï¸âƒ£ [Satu-source](https://github.com/Satu-source)
  ğŸ“Š Data views and loading into Mongo
- *ï¸âƒ£ [JKin-0](https://github.com/JKin-0)
  ğŸ§¹ Data preparation and filtering
- *ï¸âƒ£ [psword](https://github.com/psword)
  ğŸ§ª Colab testing between and GitHub host
---

## The driver :car: for this repository:

ğŸ“šFinal Project: Big Data Pipeline with Apache Spark & MongoDB

ğŸš€In final project, you will design and implement a Big Data processing pipeline using Apache Spark and MongoDB. The project will involve ingesting, processing, analyzing, and storing large-scale datasets to derive meaningful insights. You will demonstrate their understanding of data engineering, query optimization, and integration between Spark and MongoDB.

#### <ins>Requirements</ins>

Each team must:

âœ” Choose a real-world dataset (e.g., Social Media, IoT, Financial, Healthcare, E-Commerce).

âœ” Store the dataset in MongoDB, utilizing an optimized schema design.

âœ” Process and analyze the data using Apache Spark (PySpark).

âœ” Use Spark SQL for querying the dataset, comparing performance with MongoDB Aggregation Framework.

âœ” Optimize performance using indexing, sharding, and partitioning techniques in MongoDB.

âœ” Visualize key insights using Python libraries such as Matplotlib or Seaborn.

âœ” Present findings in a final report and a class presentation.

#### <ins>Deliverables</ins>

__Project Proposal (1-page summary) â€“ Dataset choice, objectives, expected outcomes. (Due: Week 3)__

- Implementation & Codebase â€“ A well-structured code repository (GitHub).
- Final Report (3-5 pages) â€“ Introduction, methodology, implementation, results, and conclusions.
- Final Presentation (10-15 minutes on ) â€“ Demonstration of the pipeline and key insights.
