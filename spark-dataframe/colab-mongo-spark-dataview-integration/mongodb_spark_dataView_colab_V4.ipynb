{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "view-in-github",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psword/big-data-pipeline-sparky/blob/Data-view/mongodb_spark_colab_compatible.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KRiStbgiK6fc",
      "metadata": {
        "id": "KRiStbgiK6fc"
      },
      "source": [
        "# ✅ PySpark + MongoDB Atlas (Compatible Setup for Google Colab)\n",
        "This notebook uses Apache Spark **3.3.2** and MongoDB Spark Connector **10.1.1**, which are compatible."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mongodb_spark_dataView_coöab_V4 Is compiling together:**\n",
        "- Spark code that cleans dataset and make dataFrame. (Janika did)\n",
        "- Connecting PySpark + MongoDB Atlas. (Philip did)\n",
        "- Finally Data frame change from spark DataFrame to Pandas DataFrame and some visualization codes. (Satu did + Janika helped)\n",
        "\n",
        "Then after these changes we make visualization by suing different charts and figures and for doing them we use pandas, matplotlib and seaborn.\n",
        "\n",
        "With these visualizations we can analyse our data and look different information easily."
      ],
      "metadata": {
        "id": "exuYtLhLZ-qe"
      },
      "id": "exuYtLhLZ-qe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting MongoDB and Sparks"
      ],
      "metadata": {
        "id": "e6r-H4t2UgkP"
      },
      "id": "e6r-H4t2UgkP"
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Step 1: Install Python dependencies\n",
        "!pip install -q pyspark findspark pymongo"
      ],
      "metadata": {
        "id": "dFWqiplYc9xv"
      },
      "id": "dFWqiplYc9xv",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "nCq3pf8yK6fk",
      "metadata": {
        "id": "nCq3pf8yK6fk"
      },
      "outputs": [],
      "source": [
        "# ⚙️ Step 2: Install Java and Spark 3.3.2\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "iGpHKEKTK6fl",
      "metadata": {
        "id": "iGpHKEKTK6fl"
      },
      "outputs": [],
      "source": [
        "# 🌱 Step 3: Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "os.environ[\"MONGO_URI\"] = \"mongodb+srv://satu20006:BCPvqNcXNRQo9Q3m@bigdata.kvauode.mongodb.net/tmdb?retryWrites=true&w=majority\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "lbarXYjGK6fm",
      "metadata": {
        "id": "lbarXYjGK6fm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "d5a74dd9-90c0-44d5-87d5-f6f53c5f5132"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "[Errno 111] Connection refused",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3139226d8c2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.mongodb.read.connection.uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MONGO_URI\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.mongodb.write.connection.uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MONGO_URI\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     getattr(\n\u001b[0;32m--> 275\u001b[0;31m                         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SparkSession$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MODULE$\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                     ).applyModifiableSettings(session._jsparkSession, self._options)\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1710\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ],
      "source": [
        "# 🚀 Step 4: Start Spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBIntegration\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\") \\\n",
        "    .config(\"spark.mongodb.read.connection.uri\", os.environ[\"MONGO_URI\"]) \\\n",
        "    .config(\"spark.mongodb.write.connection.uri\", os.environ[\"MONGO_URI\"]) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Qj53OBmaK6fm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj53OBmaK6fm",
        "outputId": "48412b74-e44d-454b-fe2a-f214f442fc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data from MongoDB:\n",
            "+--------------------+-----+--------------------+---------+--------------------+--------------------+------+---------+--------------------+-----------------+---------------+--------------------+----------+--------------------+--------------------+--------------------+------------+-----------+-------+--------------------+--------+--------------------+---------------+------------+----------+\n",
            "|                 _id|adult|       backdrop_path|   budget|              genres|            homepage|    id|  imdb_id|            keywords|original_language| original_title|            overview|popularity|         poster_path|production_companies|production_countries|release_date|    revenue|runtime|    spoken_languages|  status|             tagline|          title|vote_average|vote_count|\n",
            "+--------------------+-----+--------------------+---------+--------------------+--------------------+------+---------+--------------------+-----------------+---------------+--------------------+----------+--------------------+--------------------+--------------------+------------+-----------+-------+--------------------+--------+--------------------+---------------+------------+----------+\n",
            "|67efc28f2da50f02e...|False|/8ZTVqvKDQ8emSGUE...|160000000|Action, Science F...|https://www.warne...| 27205|tt1375666|rescue, mission, ...|               en|      Inception|Cobb, a skilled t...|    83.952|/oYuLEt3zVCKq57qu...|Legendary Picture...|United Kingdom, U...|  2010-07-15|  825532764|    148|English, French, ...|Released|Your mind is the ...|      Inception|       8.364|     34495|\n",
            "|67efc28f2da50f02e...|False|/nMKdUUepR0i5zn0y...|185000000|Drama, Action, Cr...|https://www.warne...|   155|tt0468569|joker, sadism, ch...|               en|The Dark Knight|Batman raises the...|   130.643|/qJ2tW6WMUDux911r...|DC Comics, Legend...|United Kingdom, U...|  2008-07-16| 1004558444|    152|   English, Mandarin|Released|Welcome to a worl...|The Dark Knight|       8.512|     30619|\n",
            "|67efc28f2da50f02e...|False|/pbrkL804c8yAv3zB...|165000000|Adventure, Drama,...|http://www.inters...|157336|tt0816692|rescue, future, s...|               en|   Interstellar|The adventures of...|   140.241|/gEU2QniE6E77NI6l...|Legendary Picture...|United Kingdom, U...|  2014-11-05|  701729206|    169|             English|Released|Mankind was born ...|   Interstellar|       8.417|     32571|\n",
            "|67efc28f2da50f02e...|False|/vL5LR6WdxWPjLPFR...|237000000|Action, Adventure...|https://www.avata...| 19995|tt0499549|future, society, ...|               en|         Avatar|In the 22nd centu...|    79.932|/kyeqWdyUXW608qlY...|Dune Entertainmen...|United States of ...|  2009-12-15|-1371261270|    162|    English, Spanish|Released|Enter the world o...|         Avatar|       7.573|     29815|\n",
            "|67efc28f2da50f02e...|False|/en971MEXui9diirX...| 58000000|Action, Adventure...|https://www.20thc...|293660|tt1431045|superhero, anti h...|               en|       Deadpool|The origin story ...|    72.735|/zq8Cl3PNIDGU3iWN...|20th Century Fox,...|United States of ...|  2016-02-09|  783100000|    108|             English|Released|Witness the begin...|       Deadpool|       7.606|     28894|\n",
            "+--------------------+-----+--------------------+---------+--------------------+--------------------+------+---------+--------------------+-----------------+---------------+--------------------+----------+--------------------+--------------------+--------------------+------------+-----------+-------+--------------------+--------+--------------------+---------------+------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 📄 Step 5: Read from MongoDB\n",
        "df = spark.read.format(\"mongodb\") \\\n",
        "    .option(\"database\", \"tmdb\") \\\n",
        "    .option(\"collection\", \"movies\") \\\n",
        "    .load()\n",
        "# Show initial data\n",
        "print(\"Initial data from MongoDB:\")\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "NOsWupZgLBtA",
      "metadata": {
        "id": "NOsWupZgLBtA"
      },
      "outputs": [],
      "source": [
        "# ⚙️ Step 6: Import Additional\n",
        "from pyspark.sql.functions import col, desc\n",
        "\n",
        "\n",
        "# Untill this it's modngo and spark that Philip did"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning\n",
        "\n",
        "This part if from Janikas Spark code"
      ],
      "metadata": {
        "id": "RQEuD5REUYFO"
      },
      "id": "RQEuD5REUYFO"
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7 Data cleaning\n",
        "\n",
        "# Import libraries\n",
        "import os.path\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, desc, round, date_format, format_number, regexp_replace\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "#Reading data from mongoDB again just in case and change name from df to df_movies\n",
        "df_movies = spark.read.format(\"mongodb\") \\\n",
        "    .option(\"database\", \"tmdb\") \\\n",
        "    .option(\"collection\", \"movies\") \\\n",
        "    .load()\n",
        "# Show initial data\n",
        "print(\"Initial data from MongoDB:\")\n",
        "df.show(5)\n",
        "\n",
        "# Convert minutes to hours and minutes\n",
        "def convert_to_hours_minutes(runtime):\n",
        "    if runtime is not None:\n",
        "        hours, minutes = divmod(runtime, 60)\n",
        "        return f'{hours}:{minutes:02d}'\n",
        "    return None\n",
        "\n",
        "# Define UDF\n",
        "convert_udf = udf(convert_to_hours_minutes, StringType())\n",
        "\n",
        "print(\"Step 7 first box is done\")\n",
        "df_movies.select(\"runtime\").show()\n",
        "print(type(df_movies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjExZRl9UE9k",
        "outputId": "8c62c526-a346-4923-81d2-485591489c38"
      },
      "id": "EjExZRl9UE9k",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data from MongoDB:\n",
            "+--------------------+-----+--------------------+---------+--------------------+--------------------+------+---------+--------------------+-----------------+---------------+--------------------+----------+--------------------+--------------------+--------------------+------------+-----------+-------+--------------------+--------+--------------------+---------------+------------+----------+\n",
            "|                 _id|adult|       backdrop_path|   budget|              genres|            homepage|    id|  imdb_id|            keywords|original_language| original_title|            overview|popularity|         poster_path|production_companies|production_countries|release_date|    revenue|runtime|    spoken_languages|  status|             tagline|          title|vote_average|vote_count|\n",
            "+--------------------+-----+--------------------+---------+--------------------+--------------------+------+---------+--------------------+-----------------+---------------+--------------------+----------+--------------------+--------------------+--------------------+------------+-----------+-------+--------------------+--------+--------------------+---------------+------------+----------+\n",
            "|67efc28f2da50f02e...|False|/8ZTVqvKDQ8emSGUE...|160000000|Action, Science F...|https://www.warne...| 27205|tt1375666|rescue, mission, ...|               en|      Inception|Cobb, a skilled t...|    83.952|/oYuLEt3zVCKq57qu...|Legendary Picture...|United Kingdom, U...|  2010-07-15|  825532764|    148|English, French, ...|Released|Your mind is the ...|      Inception|       8.364|     34495|\n",
            "|67efc28f2da50f02e...|False|/nMKdUUepR0i5zn0y...|185000000|Drama, Action, Cr...|https://www.warne...|   155|tt0468569|joker, sadism, ch...|               en|The Dark Knight|Batman raises the...|   130.643|/qJ2tW6WMUDux911r...|DC Comics, Legend...|United Kingdom, U...|  2008-07-16| 1004558444|    152|   English, Mandarin|Released|Welcome to a worl...|The Dark Knight|       8.512|     30619|\n",
            "|67efc28f2da50f02e...|False|/pbrkL804c8yAv3zB...|165000000|Adventure, Drama,...|http://www.inters...|157336|tt0816692|rescue, future, s...|               en|   Interstellar|The adventures of...|   140.241|/gEU2QniE6E77NI6l...|Legendary Picture...|United Kingdom, U...|  2014-11-05|  701729206|    169|             English|Released|Mankind was born ...|   Interstellar|       8.417|     32571|\n",
            "|67efc28f2da50f02e...|False|/vL5LR6WdxWPjLPFR...|237000000|Action, Adventure...|https://www.avata...| 19995|tt0499549|future, society, ...|               en|         Avatar|In the 22nd centu...|    79.932|/kyeqWdyUXW608qlY...|Dune Entertainmen...|United States of ...|  2009-12-15|-1371261270|    162|    English, Spanish|Released|Enter the world o...|         Avatar|       7.573|     29815|\n",
            "|67efc28f2da50f02e...|False|/en971MEXui9diirX...| 58000000|Action, Adventure...|https://www.20thc...|293660|tt1431045|superhero, anti h...|               en|       Deadpool|The origin story ...|    72.735|/zq8Cl3PNIDGU3iWN...|20th Century Fox,...|United States of ...|  2016-02-09|  783100000|    108|             English|Released|Witness the begin...|       Deadpool|       7.606|     28894|\n",
            "+--------------------+-----+--------------------+---------+--------------------+--------------------+------+---------+--------------------+-----------------+---------------+--------------------+----------+--------------------+--------------------+--------------------+------------+-----------+-------+--------------------+--------+--------------------+---------------+------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Step 7 first box is done\n",
            "+-------+\n",
            "|runtime|\n",
            "+-------+\n",
            "|    148|\n",
            "|    152|\n",
            "|    169|\n",
            "|    162|\n",
            "|    108|\n",
            "|    143|\n",
            "|    139|\n",
            "|    149|\n",
            "|    154|\n",
            "|    121|\n",
            "|    142|\n",
            "|    126|\n",
            "|    152|\n",
            "|    165|\n",
            "|    142|\n",
            "|    181|\n",
            "|    136|\n",
            "|    122|\n",
            "|    179|\n",
            "|    194|\n",
            "+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning it self happens here and this we need to copy when we have some other criterias clean than is done in this one\n",
        "\n",
        "# Turn \"adult\" column into boolean\n",
        "df_movies = df_movies.withColumn('adult', col('adult').cast('boolean'))\n",
        "\n",
        "# Round average vote score to 2 decimals\n",
        "df_movies = df_movies.withColumn('vote_average2', round(df_movies.vote_average, 2))\n",
        "\n",
        "# Reformat release date and runtime\n",
        "df_movies = df_movies.withColumn('release_date', date_format('release_date', 'dd/MM/yyyy'))\n",
        "#df_movies = df_movies.withColumn('runtime', convert_udf(col('runtime')))\n",
        "\n",
        "# Reorder columns\n",
        "df_movies = df_movies.select('title', 'vote_average2', 'release_date', 'revenue',\n",
        "                            'budget', 'runtime', 'genres', 'production_companies')\n",
        "\n",
        "# Rename columns\n",
        "df_movies = df_movies.withColumnRenamed('title', 'Title') \\\n",
        "    .withColumnRenamed('vote_average2', 'Average Vote Score') \\\n",
        "    .withColumnRenamed('release_date', 'Release Date') \\\n",
        "    .withColumnRenamed('revenue', 'Revenue') \\\n",
        "    .withColumnRenamed('budget', 'Budget') \\\n",
        "    .withColumnRenamed('runtime', 'Length') \\\n",
        "    .withColumnRenamed('genres', 'Genres') \\\n",
        "    .withColumnRenamed('production_companies', 'Production Companies')\n",
        "\n",
        "# Drop unneeded columns\n",
        "df_movies = df_movies.drop('id', 'vote_average', 'vote_count', 'status', 'adult',\n",
        "                'backdrop_path', 'homepage', 'imdb_id', 'original_language',\n",
        "                'original_title', 'overview', 'popularity', 'poster_path',\n",
        "                'tagline', 'production_countries', 'spoken_languages', 'keywords')\n",
        "\n",
        "print(type(df_movies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksUsBY2WVCW7",
        "outputId": "b5d5328a-90ac-4d8a-e698-5373796bdb6b"
      },
      "id": "ksUsBY2WVCW7",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization codes"
      ],
      "metadata": {
        "id": "ZkS4fvCNUcIv"
      },
      "id": "ZkS4fvCNUcIv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8 Have many styles to convers Spark DataFrame into Pandas DtataFrame, For most we can use df_movies but I still keep other styles there as comments so if needed we can use those also.\n",
        "\n",
        "There is also limit now that its avoiding loading too much data with that command df.limit(1000), this way code is littlebit faster"
      ],
      "metadata": {
        "id": "Q1-E5n7hVktv"
      },
      "id": "Q1-E5n7hVktv"
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df_movies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iB2sRk6f6xl",
        "outputId": "b7914471-a5b9-4eb2-f500-04a94d6820c6"
      },
      "id": "-iB2sRk6f6xl",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "wp6zjbt3Uszb",
      "metadata": {
        "id": "wp6zjbt3Uszb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "051bcbc3-0517-4866-e96f-f426a237997a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o253.collectToPython.\n: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioning failed. Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:77)\n\tat com.mongodb.spark.sql.connector.read.MongoBatch.planInputPartitions(MongoBatch.java:54)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions$lzycompute(BatchScanExec.scala:54)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions(BatchScanExec.scala:54)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.supportsColumnar(BatchScanExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:143)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:117)\n\tat com.mongodb.spark.sql.connector.read.partitioner.SamplePartitioner.generatePartitions(SamplePartitioner.java:103)\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:45)\n\t... 68 more\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting for a server that matches com.mongodb.client.internal.MongoClientDelegate$1@18096e41. Client view of cluster state is {type=REPLICA_SET, servers=[{address=ac-t28aye1-shard-00-02.kvauode.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, {address=ac-t28aye1-shard-00-01.kvauode.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, {address=ac-t28aye1-shard-00-00.kvauode.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}]\n\tat com.mongodb.internal.connection.BaseCluster.createTimeoutException(BaseCluster.java:428)\n\tat com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:125)\n\tat com.mongodb.internal.connection.AbstractMultiServerCluster.selectServer(AbstractMultiServerCluster.java:54)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:146)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:133)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:90)\n\tat com.mongodb.client.internal.MongoIterableImpl.first(MongoIterableImpl.java:101)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.lambda$storageStats$0(PartitionerHelper.java:107)\n\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withCollection(AbstractMongoConfig.java:173)\n\tat com.mongodb.spark.sql.connector.config.ReadConfig.withCollection(ReadConfig.java:45)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:102)\n\t... 70 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-8bdcfc02ac84>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#viz_data = df_movies.toPandas()  # Convert PySpark DataFrame to Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#viz_data = viz_data[:1000]    # Limit to the first 1000 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mviz_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Avoid loading too much data into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Convert the PySpark DataFrame to a Pandas DataFrame when name is df_movies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \"\"\"\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o253.collectToPython.\n: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioning failed. Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:77)\n\tat com.mongodb.spark.sql.connector.read.MongoBatch.planInputPartitions(MongoBatch.java:54)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions$lzycompute(BatchScanExec.scala:54)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions(BatchScanExec.scala:54)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.supportsColumnar(BatchScanExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:143)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:117)\n\tat com.mongodb.spark.sql.connector.read.partitioner.SamplePartitioner.generatePartitions(SamplePartitioner.java:103)\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:45)\n\t... 68 more\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting for a server that matches com.mongodb.client.internal.MongoClientDelegate$1@18096e41. Client view of cluster state is {type=REPLICA_SET, servers=[{address=ac-t28aye1-shard-00-02.kvauode.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, {address=ac-t28aye1-shard-00-01.kvauode.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, {address=ac-t28aye1-shard-00-00.kvauode.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}]\n\tat com.mongodb.internal.connection.BaseCluster.createTimeoutException(BaseCluster.java:428)\n\tat com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:125)\n\tat com.mongodb.internal.connection.AbstractMultiServerCluster.selectServer(AbstractMultiServerCluster.java:54)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:146)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:133)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:90)\n\tat com.mongodb.client.internal.MongoIterableImpl.first(MongoIterableImpl.java:101)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.lambda$storageStats$0(PartitionerHelper.java:107)\n\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withCollection(AbstractMongoConfig.java:173)\n\tat com.mongodb.spark.sql.connector.config.ReadConfig.withCollection(ReadConfig.java:45)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:102)\n\t... 70 more\n"
          ]
        }
      ],
      "source": [
        "#Step 8 Conversing Spark dDtaFrame to pandas DataFrame\n",
        "\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame when name is pf_pd\n",
        "#df_pd = df.limit(1000).toPandas()  # Avoid loading too much data into memory\n",
        "\n",
        "# For larger datasets, aggregate first in Spark\n",
        "#agg_data = final_df.groupBy('genre').agg(\n",
        "#    avg('budget').alias('avg_budget'),\n",
        "#    avg('revenue').alias('avg_revenue')\n",
        "#).toPandas()\n",
        "\n",
        "# Convert the PySpark DataFrame to a Pandas DataFrame when name is viz_data\n",
        "#viz_data = df_movies.toPandas()  # Convert PySpark DataFrame to Pandas\n",
        "#viz_data = viz_data[:1000]    # Limit to the first 1000 rows\n",
        "viz_data = df.limit(1000).toPandas()  # Avoid loading too much data into memory\n",
        "\n",
        "# Convert the PySpark DataFrame to a Pandas DataFrame when name is df_movies\n",
        "df_movies = df_movies.toPandas()  # Convert PySpark DataFrame to Pandas\n",
        "df_movies = df_movies[:1000]    # Limit to the first 1000 rows\n",
        "\n",
        "# Convert the PySpark DataFrame to a Pandas DataFrame when name is pdf_movies\n",
        "pdf_movies = df_movies.toPandas()  # Convert PySpark DataFrame to Pandas\n",
        "pdf_movies = pdf_movies[:1000]    # Limit to the first 1000 rows\n",
        "\n",
        "#df_movies_limited = df_movies.limit(1000)  # Limit rows in PySpark\n",
        "#pdf_movies = df_movies_limited.toPandas()  # Then convert to Pandas\n",
        "print(\"Converting from spark DataFrame to Pandas DataFrame id done\")\n",
        "\n",
        "print(type(df_movies))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding visualization codes"
      ],
      "metadata": {
        "id": "he8-D5UEXBHk"
      },
      "id": "he8-D5UEXBHk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Chart\n",
        "\n",
        "Revenue vs Budget scatter plot"
      ],
      "metadata": {
        "id": "UgeTGxTkXErz"
      },
      "id": "UgeTGxTkXErz"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter out rows where budget or revenue is 0 or null and convert to Pandas DataFrame\n",
        "viz_data = df_movies.filter((df_movies['Budget'] > 0) & (df_movies['Revenue'] > 0)).toPandas()\n",
        "\n",
        "# Convert values to millions for better readability\n",
        "viz_data['Budget'] = viz_data['Budget'] / 1e6\n",
        "viz_data['Revenue'] = viz_data['Revenue'] / 1e6\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(viz_data['Budget'], viz_data['Revenue'], alpha=0.5, color='blue')\n",
        "plt.title(\"Budget vs Revenue Correlation\")\n",
        "plt.xlabel(\"Budget (Million USD)\")\n",
        "plt.ylabel(\"Revenue (Million USD)\")\n",
        "\n",
        "# Set axis limits from 0 to 100 million USD\n",
        "plt.xlim(0, 220)  # Budget axis: 0 to 100 million USD\n",
        "plt.ylim(0, 220)  # Revenue axis: 0 to 100 million USD\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u6r8orWgXEM7"
      },
      "id": "u6r8orWgXEM7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pie chart\n",
        "\n",
        "Top 10 years that released the most movies"
      ],
      "metadata": {
        "id": "-8pz9hFSXsOx"
      },
      "id": "-8pz9hFSXsOx"
    },
    {
      "cell_type": "code",
      "source": [
        "#Pie chart test\n",
        "import pandas as pd\n",
        "\n",
        "# VISUALIZATIONS USING toPandas()\n",
        "# ======================================================================\n",
        "\n",
        "# Data cleaning for visualization\n",
        "pdf_movies['Revenue'] = pdf_movies['Revenue'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Budget'] = pdf_movies['Budget'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Release Year'] = pd.to_datetime(pdf_movies['Release Date'], format='%d/%m/%Y').dt.year\n",
        "#pdf_movies['Runtime_min'] = pdf_movies['Length'].str.split(':').apply(lambda x: int(x[0])*60 + int(x[1]) if isinstance(x, list) else 0)\n",
        "\n",
        "top_years = pdf_movies['Release Year'].value_counts().head(10)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(top_years, labels=top_years.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Top 10 Years with Most Movies Released')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lW_drLfmXujx"
      },
      "id": "lW_drLfmXujx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tablet\n",
        "\n",
        "Table of 50 most voted movies (Title + Genre)"
      ],
      "metadata": {
        "id": "sfnBNvmRXu5p"
      },
      "id": "sfnBNvmRXu5p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importin librarys so we have all of them just in case\n",
        "import os.path\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, desc, round, date_format, format_number, regexp_replace\n",
        "from pyspark.sql.types import StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# VISUALIZATIONS USING toPandas()\n",
        "# ======================================================================\n",
        "\n",
        "#Converting to Pandas dataFrame have already done\n",
        "\n",
        "# Data cleaning for visualization\n",
        "pdf_movies['Revenue'] = pdf_movies['Revenue'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Budget'] = pdf_movies['Budget'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Release Year'] = pd.to_datetime(pdf_movies['Release Date'], format='%d/%m/%Y').dt.year\n",
        "pdf_movies['Runtime_min'] = pdf_movies['Length'].str.split(':').apply(lambda x: int(x[0])*60 + int(x[1]) if isinstance(x, list) else 0)\n",
        "\n",
        "# Making table\n",
        "most_voted = pdf_movies.nlargest(50, 'vote_count')[['Title', 'Genres']]\n",
        "print(\"50 Most Voted Movies:\")\n",
        "display(most_voted)"
      ],
      "metadata": {
        "id": "svTX0KIgYLFE"
      },
      "id": "svTX0KIgYLFE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count how many columns and rows you kept"
      ],
      "metadata": {
        "id": "QG0N9EzvYZWu"
      },
      "id": "QG0N9EzvYZWu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importin librarys so we have all of them just in case\n",
        "import os.path\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, desc, round, date_format, format_number, regexp_replace\n",
        "from pyspark.sql.types import StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# VISUALIZATIONS USING toPandas()\n",
        "# ======================================================================\n",
        "\n",
        "#Converting to Pandas dataFrame have already done\n",
        "\n",
        "# Optional: do this before cleaning to compare\n",
        "original = pd.read_csv(df)\n",
        "print(f\"Original columns: {original.shape[1]}, cleaned: {pdf_movies.shape[1]}\")\n",
        "print(f\"Original rows: {original.shape[0]}, cleaned: {pdf_movies.shape[0]}\")\n",
        "\n",
        "# Data cleaning for visualization\n",
        "pdf_movies['Revenue'] = pdf_movies['Revenue'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Budget'] = pdf_movies['Budget'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Release Year'] = pd.to_datetime(pdf_movies['Release Date'], format='%d/%m/%Y').dt.year\n",
        "pdf_movies['Runtime_min'] = pdf_movies['Length'].str.split(':').apply(lambda x: int(x[0])*60 + int(x[1]) if isinstance(x, list) else 0)\n",
        "\n",
        "#Printing result\n",
        "print(\"\\nData Cleaning Stats:\")\n",
        "print(f\"Number of columns kept: {pdf_movies.shape[1]}\")\n",
        "print(f\"Number of rows (movies) kept: {pdf_movies.shape[0]}\")"
      ],
      "metadata": {
        "id": "Uf0vi8Y4Ybfe"
      },
      "id": "Uf0vi8Y4Ybfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bar Chart\n",
        "\n",
        "Top 20 least favorite movies"
      ],
      "metadata": {
        "id": "xFSQ2zNwYpxo"
      },
      "id": "xFSQ2zNwYpxo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importin librarys so we have all of them just in case\n",
        "import os.path\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, desc, round, date_format, format_number, regexp_replace\n",
        "from pyspark.sql.types import StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# VISUALIZATIONS USING toPandas()\n",
        "# ======================================================================\n",
        "\n",
        "#Converting to Pandas dataFrame have already done\n",
        "\n",
        "# Data cleaning for visualization\n",
        "pdf_movies['Revenue'] = pdf_movies['Revenue'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Budget'] = pdf_movies['Budget'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Release Year'] = pd.to_datetime(pdf_movies['Release Date'], format='%d/%m/%Y').dt.year\n",
        "pdf_movies['Runtime_min'] = pdf_movies['Length'].str.split(':').apply(lambda x: int(x[0])*60 + int(x[1]) if isinstance(x, list) else 0)\n",
        "\n",
        "# Converting 'Average Vote Score' to a numeric value\n",
        "pdf_movies['Average Vote Score'] = pd.to_numeric(pdf_movies['Average Vote Score'], errors='coerce')\n",
        "\n",
        "# Filtering\n",
        "filtered = pdf_movies[\n",
        "    (pdf_movies['Average Vote Score'] > 0) &\n",
        "    (pdf_movies['vote_count'] >= 100) &\n",
        "    (pdf_movies['adult'] == False)\n",
        "]\n",
        "\n",
        "least_favorite = filtered.nsmallest(20, 'Average Vote Score')[['Title', 'Average Vote Score']]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(least_favorite['Title'], least_favorite['Average Vote Score'], color='orange')\n",
        "plt.title('Top 20 Least Favorite Movies with at least 100 votes')\n",
        "plt.xlabel('Average Vote Score')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F99v38VCYuqD"
      },
      "id": "F99v38VCYuqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pie Chart 2\n",
        "\n",
        "How many different languages there are"
      ],
      "metadata": {
        "id": "XqvwAvh4ZCjU"
      },
      "id": "XqvwAvh4ZCjU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importin librarys so we have all of them just in case\n",
        "port os.path\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, desc, round, date_format, format_number, regexp_replace\n",
        "from pyspark.sql.types import StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# VISUALIZATIONS USING toPandas()\n",
        "# ======================================================================\n",
        "\n",
        "#Converting to Pandas dataFrame have already done\n",
        "\n",
        "# Data cleaning for visualization\n",
        "pdf_movies['Revenue'] = pdf_movies['Revenue'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Budget'] = pdf_movies['Budget'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Release Year'] = pd.to_datetime(pdf_movies['Release Date'], format='%d/%m/%Y').dt.year\n",
        "pdf_movies['Runtime_min'] = pdf_movies['Length'].str.split(':').apply(lambda x: int(x[0])*60 + int(x[1]) if isinstance(x, list) else 0)\n",
        "\n",
        "#Making chart\n",
        "language_counts = pdf_movies['original_language'].value_counts()\n",
        "plt.figure(figsize=(10, 8))\n",
        "language_counts.head(10).plot(kind='pie', autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Top 10 Languages in Movies')\n",
        "plt.ylabel('')\n",
        "plt.show()\n",
        "\n",
        "# Total number of unique languages:\n",
        "print(f\"Total different languages: {pdf_movies['original_language'].nunique()}\")"
      ],
      "metadata": {
        "id": "fHusaafNZI2k"
      },
      "id": "fHusaafNZI2k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bar Chart in hours\n",
        "\n",
        "Top 5 movies by runtime"
      ],
      "metadata": {
        "id": "p7DJWCfOZejn"
      },
      "id": "p7DJWCfOZejn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importin librarys so we have all of them just in case\n",
        "import os.path\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, desc, round, date_format, format_number, regexp_replace\n",
        "from pyspark.sql.types import StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# VISUALIZATIONS USING toPandas()\n",
        "# ======================================================================\n",
        "\n",
        "#Converting to Pandas dataFrame have already done\n",
        "\n",
        "# Data cleaning for visualization\n",
        "pdf_movies['Revenue'] = pdf_movies['Revenue'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Budget'] = pdf_movies['Budget'].astype(str).str.replace(' ', '').replace('None', '0').astype(float)\n",
        "pdf_movies['Release Year'] = pd.to_datetime(pdf_movies['Release Date'], format='%d/%m/%Y').dt.year\n",
        "pdf_movies['Runtime_min'] = pdf_movies['Length'].str.split(':').apply(lambda x: int(x[0])*60 + int(x[1]) if isinstance(x, list) else 0)\n",
        "\n",
        "#Making chart\n",
        "top_runtime = pdf_movies.nlargest(5, 'Runtime_min')[['Title', 'Runtime_min']]\n",
        "top_runtime['Runtime_hours'] = (top_runtime['Runtime_min'] / 60)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.barh(top_runtime['Title'], top_runtime['Runtime_hours'], color='green')\n",
        "plt.title('Top 5 Longest Movies by Runtime')\n",
        "plt.xlabel('Runtime (hours)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GLZb-nP3Zjwv"
      },
      "id": "GLZb-nP3Zjwv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}